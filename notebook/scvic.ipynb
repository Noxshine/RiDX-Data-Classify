{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "\n",
    "BASE_URL = 'C:/Users/Admin/Desktop/DATN/DATA/SVIC-APT-2021/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = [f\"{BASE_URL}phase{i}.csv\" for i in range(1, 5)]\n",
    "\n",
    "# df_temp = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "# df = pd.concat(df_temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna(axis=1, how='all')\n",
    "df = pd.read_csv(BASE_URL+'Training.csv')\n",
    "df2 = pd.read_csv(BASE_URL+'Testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.value_counts('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = ['Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
    "       'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
    "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
    "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
    "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
    "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
    "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
    "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
    "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
    "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
    "       'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
    "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
    "       'Packet Length Min', 'Packet Length Max', 'Packet Length Mean',\n",
    "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
    "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
    "       'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
    "       'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
    "       'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg',\n",
    "       'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
    "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
    "       'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
    "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
    "       'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max',\n",
    "       'Idle Min', 'Label']\n",
    "\n",
    "df = df[list_features]\n",
    "df2 = df2[list_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_unique_val = []\n",
    "cols_unique_val2 = []\n",
    "\n",
    "# Dropping features contain unique value for every instance\n",
    "for i in range(1,2):\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == i:\n",
    "            cols_unique_val.append(col)\n",
    "\n",
    "for i in range(1,2):\n",
    "    for col in df.columns:\n",
    "        if df2[col].nunique() == i:\n",
    "            cols_unique_val2.append(col)\n",
    "\n",
    "print(f'Features contain unique value for every instance: {cols_unique_val}')\n",
    "print(f'Features contain unique value for every instance: {cols_unique_val2}')\n",
    "\n",
    "print(len(cols_unique_val))\n",
    "print(len(cols_unique_val2))\n",
    "\n",
    "df.drop(cols_unique_val, axis=1, inplace=True)\n",
    "df2.drop(cols_unique_val, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.value_counts('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_mapping = {\n",
    "    'NormalTraffic': 0,        \n",
    "    'Reconnaissance': 1,     \n",
    "    'InitialCompromise': 1,\n",
    "    'LateralMovement': 2,\n",
    "    'Pivoting': 2,\n",
    "    'DataExfiltration': 3\n",
    "}\n",
    "\n",
    "df['Label'] = df['Label'].map(stage_mapping)\n",
    "df2['Label'] = df2['Label'].map(stage_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.value_counts('Stage')\n",
    "df.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Dropping duplicate samples\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# df.value_counts('Stage')\n",
    "df2.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "df2.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Dropping duplicate samples\n",
    "df2.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(BASE_URL + 'scvic_training.csv')\n",
    "df2.to_csv(BASE_URL + 'scvic_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "-----------------------\n",
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_URL + 'scvic_training.csv')\n",
    "df2 = pd.read_csv(BASE_URL + 'scvic_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df2.drop('Unnamed: 0', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.iloc[:, :-1].values\n",
    "y_train = df.iloc[:, -1].values\n",
    "\n",
    "x_test = df2.iloc[:, :-1].values\n",
    "y_test = df2.iloc[:, -1].values\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "x_test, y_test = smote.fit_resample(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scvic = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scvic.fit(x_train, y_train)\n",
    "\n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Perform cross-validation\n",
    "# scores = cross_val_score(rf_scvic, x_train, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "# print(\"Cross-Validation Scores:\", scores)\n",
    "# print(\"Mean Accuracy:\", scores.mean())\n",
    "# print(\"Standard Deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_scvic.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "f2 = f1_score(y_test, y_pred, average='macro')\n",
    "f3 = f1_score(y_test, y_pred, average='micro')\n",
    "# accuracy3 = f1_score(y_test, y_pred, average='binary')\n",
    "pr1 = precision_score(y_test, y_pred, average='weighted')\n",
    "pr2 = precision_score(y_test, y_pred, average='macro')\n",
    "pr3 = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "re1 = recall_score(y_test, y_pred, average='weighted')\n",
    "re2 = recall_score(y_test, y_pred, average='macro')\n",
    "re3 = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "print(accuracy)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"f1 : \" ,f1)\n",
    "print(\"f1 : \" ,f2)\n",
    "print(\"f1 : \" ,f3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"precision : \" , pr1)\n",
    "print(\"precision : \" , pr2)\n",
    "print(\"precision : \" , pr3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"recall : \" , re1)\n",
    "print(\"recall : \" , re2)\n",
    "print(\"recall : \" , re3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scvic = LogisticRegression(\n",
    "    C=0.1,\n",
    "    solver='sag',\n",
    "    max_iter=200,\n",
    "    multi_class='multinomial'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scvic.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_scvic.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "f2 = f1_score(y_test, y_pred, average='macro')\n",
    "f3 = f1_score(y_test, y_pred, average='micro')\n",
    "# accuracy3 = f1_score(y_test, y_pred, average='binary')\n",
    "pr1 = precision_score(y_test, y_pred, average='weighted')\n",
    "pr2 = precision_score(y_test, y_pred, average='macro')\n",
    "pr3 = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "re1 = recall_score(y_test, y_pred, average='weighted')\n",
    "re2 = recall_score(y_test, y_pred, average='macro')\n",
    "re3 = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "print(accuracy)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"f1 : \" ,f1)\n",
    "print(\"f1 : \" ,f2)\n",
    "print(\"f1 : \" ,f3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"precision : \" , pr1)\n",
    "print(\"precision : \" , pr2)\n",
    "print(\"precision : \" , pr3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"recall : \" , re1)\n",
    "print(\"recall : \" , re2)\n",
    "print(\"recall : \" , re3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_scvic = xgb.XGBClassifier(objective='multi:softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_scvic.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_scvic.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "f2 = f1_score(y_test, y_pred, average='macro')\n",
    "f3 = f1_score(y_test, y_pred, average='micro')\n",
    "# accuracy3 = f1_score(y_test, y_pred, average='binary')\n",
    "pr1 = precision_score(y_test, y_pred, average='weighted')\n",
    "pr2 = precision_score(y_test, y_pred, average='macro')\n",
    "pr3 = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "re1 = recall_score(y_test, y_pred, average='weighted')\n",
    "re2 = recall_score(y_test, y_pred, average='macro')\n",
    "re3 = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "print(accuracy)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"f1 : \" ,f1)\n",
    "print(\"f1 : \" ,f2)\n",
    "print(\"f1 : \" ,f3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"precision : \" , pr1)\n",
    "print(\"precision : \" , pr2)\n",
    "print(\"precision : \" , pr3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"recall : \" , re1)\n",
    "print(\"recall : \" , re2)\n",
    "print(\"recall : \" , re3)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zz = df2[df2['Label'] == 3]\n",
    "df_zz.iloc[0]\n",
    "t1 = df_zz.iloc[0, :-1].values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z2 = df2[df2['Label'] == 2]\n",
    "df_z2.iloc[4:10]\n",
    "t2 = df_z2.iloc[:, :-1].values\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_scvic.predict(t2)\n",
    "\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(xgb_scvic, 'xgb_svic.joblib')\n",
    "\n",
    "\n",
    "# joblib.dump(rf_scvic, 'random_forest_scvic.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
