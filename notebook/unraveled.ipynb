{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", \"r\") as yamlfile:\n",
    "    cfg = yaml.safe_load(yamlfile)\n",
    "\n",
    "for section in cfg:\n",
    "    RANDOM_STATE_SEED = cfg[\"variables\"][\"RANDOM_STATE_SEED\"]\n",
    "    THRESHOLD = cfg[\"variables\"][\"THRESHOLD\"]\n",
    "    THRESHOLD_LOSS = cfg[\"variables\"][\"THRESHOLD_LOSS\"]\n",
    "\n",
    "    PROCESSED_DATA_DIR = cfg[\"data_path\"][\"PROCESSED_DATA_DIR\"]\n",
    "\n",
    "DATA_DIR = 'C:/Users/Admin/Desktop/DATN/DATA/Unraveled_DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [os.path.join(DATA_DIR,  dataset_path) for dataset_path in os.listdir(DATA_DIR)]\n",
    "all_csv_files = []\n",
    "for dataset_path in dataset_paths:\n",
    "    csv_files_directory = glob.glob(dataset_path + '/*.csv')\n",
    "    all_csv_files.extend(csv_files_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dataframes = {}\n",
    "\n",
    "if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "    os.mkdir(PROCESSED_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for csv_file in all_csv_files:\n",
    "#     convert_csv_to_dataframe(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of apt2021 files: {len(all_csv_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt_files = []\n",
    "def get_apt_files(csv_file, threshold):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if (df['Stage'].value_counts()['Benign'] / len(df)) * 100 < threshold:\n",
    "        print(f'File: {csv_file}')\n",
    "        apt_files.append(csv_file)\n",
    "        print('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file in all_csv_files:\n",
    "    get_apt_files(csv_file, threshold=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(apt_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = [pd.read_csv(file_path) for file_path in all_csv_files]\n",
    "df = pd.concat(df_temp, ignore_index=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Stage'] != 'Normal']\n",
    "print(df['Stage'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('Stage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete random Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Get the indices of rows labeled as \"Benign\"\n",
    "# benign_indices = df[df['Stage'] == 'Benign'].index\n",
    "\n",
    "# # Randomly select indices to delete\n",
    "# indices_to_delete = np.random.choice(benign_indices, 200000, replace=False)\n",
    "\n",
    "# # Drop rows with selected indices\n",
    "# df = df.drop(indices_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = [\n",
    "    'bidirectional_duration_ms', 'src2dst_duration_ms',\n",
    "    'dst2src_duration_ms', 'src2dst_packets', 'dst2src_packets',\n",
    "    'src2dst_bytes', 'dst2src_bytes', 'src2dst_max_ps', 'src2dst_min_ps',\n",
    "    'src2dst_mean_ps', 'src2dst_stddev_ps', 'dst2src_max_ps', 'dst2src_min_ps',\n",
    "    'dst2src_mean_ps', 'dst2src_stddev_ps', 'bidirectional_mean_piat_ms',\n",
    "    'bidirectional_stddev_piat_ms', 'bidirectional_max_piat_ms', \n",
    "    'bidirectional_min_piat_ms','src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms', \n",
    "    'src2dst_max_piat_ms', 'src2dst_min_piat_ms', 'dst2src_mean_piat_ms',\n",
    "    'dst2src_stddev_piat_ms', 'dst2src_max_piat_ms', 'dst2src_min_piat_ms', \n",
    "    'bidirectional_fin_packets', 'src2dst_fin_packets', 'dst2src_fin_packets', \n",
    "    'bidirectional_syn_packets', 'src2dst_syn_packets', 'dst2src_syn_packets',\n",
    "    'bidirectional_rst_packets', 'src2dst_rst_packets', 'dst2src_rst_packets',\n",
    "    'bidirectional_psh_packets', 'src2dst_psh_packets', 'dst2src_psh_packets', \n",
    "    'bidirectional_ack_packets', 'src2dst_ack_packets', 'dst2src_ack_packets', \n",
    "    'bidirectional_urg_packets', 'src2dst_urg_packets', 'dst2src_urg_packets',\n",
    "    'bidirectional_cwr_packets', 'src2dst_cwr_packets', 'dst2src_cwr_packets', \n",
    "    'bidirectional_ece_packets', 'src2dst_ece_packets', 'dst2src_ece_packets',\n",
    "    'DefenderResponse'\n",
    "]\n",
    "\n",
    "df = df[list_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove column unique value < 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('C:/Users/Admin/Desktop/DATN/DATA/Unraveled_DATA2/W5_D6/net3.csv',on_bad_lines='skip')\n",
    "cols_unique_val = []\n",
    "\n",
    "# Dropping features contain unique value for every instance\n",
    "# for i in range (1,3):\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() == 1:\n",
    "        cols_unique_val.append(col)\n",
    "        \n",
    "print(f'Features contain unique value for every instance: {cols_unique_val}')\n",
    "df.drop(cols_unique_val, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stage_mapping = {\n",
    "    'Benign': 0,        \n",
    "    'Reconnaissance': 1,     \n",
    "    'Establish Foothold': 2,\n",
    "    'Lateral Movement': 2,\n",
    "    'Data Exfiltration': 3,\n",
    "    'Cover up': 3\n",
    "\n",
    "}\n",
    "\n",
    "df['Stage'] = df['Stage'].map(stage_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stage_mapping = {\n",
    "    'Benign': 0,        \n",
    "    'Detected': 1,     \n",
    "}\n",
    "\n",
    "df['DefenderResponse'] = df['DefenderResponse'].map(stage_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('DefenderResponse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# df.value_counts('Stage')\n",
    "df.replace([-np.inf, np.inf], np.nan, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Dropping duplicate samples\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.value_counts('DefenderResponse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/Admin/Desktop/DATN/DATA/unraveled_defender_response.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------\n",
    "-----------------------------------------------\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Admin/Desktop/DATN/DATA/unraveled_defender_response.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.value_counts('DefenderResponse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training ATTACK</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = df.drop('Stage', axis=1)\n",
    "Y_train = df['Stage']\n",
    "\n",
    "# x_train = df.iloc[:, :-1].values\n",
    "# y_train = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, stratify=Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "x_test, y_test = smote.fit_resample(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_apt2021 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_apt2021.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ft = ['bidirectional_duration_ms', 'src2dst_duration_ms',\n",
    "       'dst2src_duration_ms', 'src2dst_packets', 'dst2src_packets',\n",
    "       'src2dst_bytes', 'dst2src_bytes', 'src2dst_max_ps', 'src2dst_min_ps',\n",
    "       'src2dst_mean_ps', 'src2dst_stddev_ps', 'dst2src_max_ps',\n",
    "       'dst2src_min_ps', 'dst2src_mean_ps', 'dst2src_stddev_ps',\n",
    "       'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms',\n",
    "       'bidirectional_max_piat_ms', 'bidirectional_min_piat_ms',\n",
    "       'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms', 'src2dst_max_piat_ms',\n",
    "       'src2dst_min_piat_ms', 'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms',\n",
    "       'dst2src_max_piat_ms', 'dst2src_min_piat_ms',\n",
    "       'bidirectional_fin_packets', 'src2dst_fin_packets',\n",
    "       'dst2src_fin_packets', 'bidirectional_syn_packets',\n",
    "       'src2dst_syn_packets', 'dst2src_syn_packets',\n",
    "       'bidirectional_rst_packets', 'src2dst_rst_packets',\n",
    "       'dst2src_rst_packets', 'bidirectional_psh_packets',\n",
    "       'src2dst_psh_packets', 'dst2src_psh_packets',\n",
    "       'bidirectional_ack_packets', 'src2dst_ack_packets',\n",
    "       'dst2src_ack_packets', 'bidirectional_urg_packets',\n",
    "       'src2dst_urg_packets']\n",
    "\n",
    "df_test = pd.read_csv(\"C:/Users/Admin/Desktop/DATN/DATA/unraveled-test.csv\")\n",
    "df_test = df_test[list_ft]\n",
    "df_test\n",
    "df_test = df_test.values\n",
    "y_test_pred = random_forest_apt2021.predict(df_test)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "y_pred = random_forest_apt2021.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('Stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "f2 = f1_score(y_test, y_pred, average='macro')\n",
    "f3 = f1_score(y_test, y_pred, average='micro')\n",
    "# accuracy3 = f1_score(y_test, y_pred, average='binary')\n",
    "pr1 = precision_score(y_test, y_pred, average='weighted')\n",
    "pr2 = precision_score(y_test, y_pred, average='macro')\n",
    "pr3 = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "re1 = recall_score(y_test, y_pred, average='weighted')\n",
    "re2 = recall_score(y_test, y_pred, average='macro')\n",
    "re3 = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "print(f1)\n",
    "print(f2)\n",
    "print(f3)\n",
    "print(\"---------------------\")\n",
    "print(pr1)\n",
    "print(pr2)\n",
    "print(pr3)\n",
    "print(\"---------------------\")\n",
    "\n",
    "print(re1)\n",
    "print(re2)\n",
    "print(re3)\n",
    "\n",
    "\n",
    "# print(accuracy3)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_apt2021 = xgb.XGBClassifier(objective='multi:softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_apt2021.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_apt2021.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "f2 = f1_score(y_test, y_pred, average='macro')\n",
    "f3 = f1_score(y_test, y_pred, average='micro')\n",
    "# accuracy3 = f1_score(y_test, y_pred, average='binary')\n",
    "pr1 = precision_score(y_test, y_pred, average='weighted')\n",
    "pr2 = precision_score(y_test, y_pred, average='macro')\n",
    "pr3 = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "re1 = recall_score(y_test, y_pred, average='weighted')\n",
    "re2 = recall_score(y_test, y_pred, average='macro')\n",
    "re3 = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"f1 : \" ,f1)\n",
    "print(\"f1 : \" ,f2)\n",
    "print(\"f1 : \" ,f3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"precision : \" , pr1)\n",
    "print(\"precision : \" , pr2)\n",
    "print(\"precision : \" , pr3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"recall : \" , re1)\n",
    "print(\"recall : \" , re2)\n",
    "print(\"recall : \" , re3)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(xgb_scvic, x_train, y_train, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-Validation Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "print(\"Standard Deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ft = ['bidirectional_duration_ms', 'src2dst_duration_ms',\n",
    "       'dst2src_duration_ms', 'src2dst_packets', 'dst2src_packets',\n",
    "       'src2dst_bytes', 'dst2src_bytes', 'src2dst_max_ps', 'src2dst_min_ps',\n",
    "       'src2dst_mean_ps', 'src2dst_stddev_ps', 'dst2src_max_ps',\n",
    "       'dst2src_min_ps', 'dst2src_mean_ps', 'dst2src_stddev_ps',\n",
    "       'bidirectional_mean_piat_ms', 'bidirectional_stddev_piat_ms',\n",
    "       'bidirectional_max_piat_ms', 'bidirectional_min_piat_ms',\n",
    "       'src2dst_mean_piat_ms', 'src2dst_stddev_piat_ms', 'src2dst_max_piat_ms',\n",
    "       'src2dst_min_piat_ms', 'dst2src_mean_piat_ms', 'dst2src_stddev_piat_ms',\n",
    "       'dst2src_max_piat_ms', 'dst2src_min_piat_ms',\n",
    "       'bidirectional_fin_packets', 'src2dst_fin_packets',\n",
    "       'dst2src_fin_packets', 'bidirectional_syn_packets',\n",
    "       'src2dst_syn_packets', 'dst2src_syn_packets',\n",
    "       'bidirectional_rst_packets', 'src2dst_rst_packets',\n",
    "       'dst2src_rst_packets', 'bidirectional_psh_packets',\n",
    "       'src2dst_psh_packets', 'dst2src_psh_packets',\n",
    "       'bidirectional_ack_packets', 'src2dst_ack_packets',\n",
    "       'dst2src_ack_packets', 'bidirectional_urg_packets',\n",
    "       'src2dst_urg_packets']\n",
    "\n",
    "df_test = pd.read_csv(\"C:/Users/Admin/Desktop/DATN/DATA/unraveled-test.csv\")\n",
    "df_test = df_test[list_ft]\n",
    "df_test\n",
    "df_test = df_test.values\n",
    "y_test_pred = xgb_apt2021.predict(df_test)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(random_forest_apt2021, 'random_forest_apt2021.pkl')\n",
    "joblib.dump(xgb_apt2021, 'xgb_apt2021.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# accuracy_before = rf.score(X_test, y_test)\n",
    "# print(f'Accuracy before feature selection: {accuracy_before:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = random_forest_apt2021.feature_importances_\n",
    "feature_names  = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Rank features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['Stage']=='Data Exfiltration']\n",
    "# # df = df[df['Stage']=='Establish Foothold']\n",
    "# # df = df[df['Stage']=='Benign']\n",
    "\n",
    "\n",
    "# # # Step 3: Plot the data\n",
    "# plt.figure(figsize=(20, 6))\n",
    "# plt.scatter(df['bidirectional_min_piat_ms'], df['Stage'], color='skyblue')\n",
    "\n",
    "# # Add titles and labels\n",
    "# plt.title('Value by Label')\n",
    "# plt.xlabel('bidirectional_duration_ms')\n",
    "# plt.ylabel('Label')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['src2dst_bytes'].max())\n",
    "# print((df['src2dst_bytes'] > 100000).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> TRAINING DEFENDER RESPONSE </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Admin/Desktop/DATN/DATA/unraveled_defender_response.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts('DefenderResponse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop('DefenderResponse', axis=1)\n",
    "Y_train = df['DefenderResponse']\n",
    "\n",
    "# x_train = df.iloc[:, :-1].values\n",
    "# y_train = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, stratify=Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "x_test, y_test = smote.fit_resample(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # 'Logistic_Regression': (\n",
    "    #     LogisticRegression(),\n",
    "    #     {\n",
    "    #         'C': [0.1, 1.0, 10.0, 25.0],\n",
    "    #         'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag'],\n",
    "    #         'max_iter': [100, 200]\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'K Nearest Neighbors': (\n",
    "    #     KNeighborsClassifier(),\n",
    "    #     {\n",
    "    #         'n_neighbors': [5, 25, 50, 100],\n",
    "    #         'weights': ['uniform', 'distance'],\n",
    "    #         'p': [1, 2]\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'Support Vector Machine': (\n",
    "    #     LinearSVC(),\n",
    "    #     {\n",
    "    #         'penalty': ['l1', 'l2'],\n",
    "    #         'loss': ['hinge', 'squared_hinge'],\n",
    "    #         'C': [0.1, 1.0, 10.0, 25.0],\n",
    "    #         'max_iter': [1000, 2000]\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'Naive Bayes': (\n",
    "    #     MultinomialNB(),\n",
    "    #      {\n",
    "    #         'alpha': [0.1, 1.0, 10.0],\n",
    "    #         'fit_prior': [True, False]\n",
    "    #      }\n",
    "    # ),\n",
    "\n",
    "    # 'Decision Tree': (\n",
    "    #     DecisionTreeClassifier(),\n",
    "    #     {\n",
    "    #         'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    #         'splitter': ['best', 'random'],\n",
    "    #         'min_samples_split': [2, 5, 7],\n",
    "    #         'min_samples_leaf': [1, 3]\n",
    "    #     }\n",
    "\n",
    "    # ),\n",
    "    'Random Forest': (\n",
    "        RandomForestClassifier(),\n",
    "        {\n",
    "            'n_estimators': [50, 100],\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'min_samples_split': [2, 5, 7],\n",
    "            'min_samples_leaf': [1, 3]\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(models):\n",
    "\n",
    "  best_models = {}\n",
    "  for model_name, model in models.items():\n",
    "    params = model[1]\n",
    "    model = model[0]\n",
    "    try:\n",
    "      grid_search = GridSearchCV(model, params, cv=5, scoring='accuracy')\n",
    "      grid_search.fit(x_train, y_train)\n",
    "      print(model_name)\n",
    "      print(\"--------------------------------------------------------------------------\")\n",
    "      print('best score: ', grid_search.best_score_)\n",
    "      print('best parameters: ', grid_search.best_params_)\n",
    "      print(\"--------------------------------------------------------------------------\", '\\n')\n",
    "      best_models[model_name] = (grid_search.best_estimator_, grid_search.best_params_)\n",
    "    except Exception as e:\n",
    "      print(f'{model_name}: {e}')\n",
    "\n",
    "  return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# xgb_def = xgb.XGBClassifier(objective='multi:softmax')\n",
    "# xgb_def.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_def = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_def.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_forest_def.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "f2 = f1_score(y_test, y_pred, average='macro')\n",
    "f3 = f1_score(y_test, y_pred, average='micro')\n",
    "# accuracy3 = f1_score(y_test, y_pred, average='binary')\n",
    "pr1 = precision_score(y_test, y_pred, average='weighted')\n",
    "pr2 = precision_score(y_test, y_pred, average='macro')\n",
    "pr3 = precision_score(y_test, y_pred, average='micro')\n",
    "\n",
    "re1 = recall_score(y_test, y_pred, average='weighted')\n",
    "re2 = recall_score(y_test, y_pred, average='macro')\n",
    "re3 = recall_score(y_test, y_pred, average='micro')\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"f1 : \" ,f1)\n",
    "print(\"f1 : \" ,f2)\n",
    "print(\"f1 : \" ,f3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"precision : \" , pr1)\n",
    "print(\"precision : \" , pr2)\n",
    "print(\"precision : \" , pr3)\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"recall : \" , re1)\n",
    "print(\"recall : \" , re2)\n",
    "print(\"recall : \" , re3)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(random_forest_apt2021, 'random_forest_apt2021.pkl')\n",
    "joblib.dump(random_forest_def, 'random_forest_unraveled_def.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
